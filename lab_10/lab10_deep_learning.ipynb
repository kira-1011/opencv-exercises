{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 10: Introduction to Deep Learning for Vision (PyTorch / TensorFlow)\n",
        "\n",
        "## Objective\n",
        "To introduce the fundamentals of deep learning in computer vision using Convolutional Neural Networks (CNNs) and pretrained models. Students will learn image classification workflows using PyTorch or TensorFlow and apply them to real-world images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is Deep Learning for Vision?\n",
        "\n",
        "**Description**: Deep learning enables automated feature extraction from images using multi-layer neural networks, especially **Convolutional Neural Networks (CNNs)**, which are designed to handle spatial data like images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment Setup\n",
        "\n",
        "### 2.1 Required Libraries\n",
        "Choose either **PyTorch** or **TensorFlow** as your preferred framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base libraries imported successfully!\n",
            "Choose either PyTorch or TensorFlow for the following sections.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run once if needed)\n",
        "# pip install torch torchvision torchaudio matplotlib\n",
        "# pip install tensorflow keras opencv-python\n",
        "\n",
        "# Import libraries for both frameworks\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "print(\"Base libraries imported successfully!\")\n",
        "print(\"Choose either PyTorch or TensorFlow for the following sections.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. CNN-Based Image Classification with PyTorch\n",
        "\n",
        "### 3.1 Using Pretrained ResNet for Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\ComputerVision\\part-II\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "e:\\ComputerVision\\part-II\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class index: 282\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pretrained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "img = Image.open('images/cat.jpeg')\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "\n",
        "print('Predicted class index:', predicted.item())\n",
        "\n",
        "# Output Description: Classifies the input image using ResNet-18 and outputs the predicted class index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CNN-Based Image Classification with TensorFlow/Keras\n",
        "\n",
        "### 4.1 Using Pretrained MobileNetV2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
            "\u001b[1m14536120/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step\n",
            "Predicted: [('n02124075', 'Egyptian_cat', np.float32(0.1570049)), ('n02123597', 'Siamese_cat', np.float32(0.07840525)), ('n02091244', 'Ibizan_hound', np.float32(0.03208647))]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "model = MobileNetV2(weights='imagenet')\n",
        "img = image.load_img('images/cat.jpeg', target_size=(224, 224))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
        "\n",
        "# Output Description: Displays the top-3 predicted labels and probabilities using MobileNetV2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Custom Image Input with OpenCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread('images/cat.jpeg')\n",
        "img = cv2.resize(img, (224, 224))\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "# Continue with same preprocessing and prediction as above\n",
        "\n",
        "# Output Description: Enables using OpenCV images as inputs for CNNs in both frameworks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizing CNN Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(img)\n",
        "plt.title(f'Predicted: {decode_predictions(preds, top=1)[0][0][1]}')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Output Description: Displays the input image with the top predicted class label.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "• **CNNs** automate feature extraction using layers of convolutions and activations.\n",
        "• **Pretrained models** like ResNet and MobileNet reduce training time and improve accuracy.\n",
        "• **PyTorch and TensorFlow** provide easy access to top-performing models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Suggested Exercises\n",
        "\n",
        "1. Run predictions on your own image dataset.\n",
        "2. Compare predictions from ResNet and MobileNet.\n",
        "3. Fine-tune a pretrained model on a small custom dataset.\n",
        "4. Visualize activation maps or intermediate feature layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Suggested Exercises Implementation\n",
        "\n",
        "## Exercise 1: Run predictions on your own image dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Load CIFAR-10 dataset and run predictions\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications.mobilenet_v2 import decode_predictions\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# CIFAR-10 class names\n",
        "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "print(f\"📊 CIFAR-10 Dataset Loaded:\")\n",
        "print(f\"   • Training images: {x_train.shape[0]}\")\n",
        "print(f\"   • Test images: {x_test.shape[0]}\")\n",
        "print(f\"   • Image size: {x_train.shape[1]}x{x_train.shape[2]} pixels\")\n",
        "print(f\"   • Classes: {len(cifar10_classes)}\")\n",
        "\n",
        "# Select a few test images for prediction\n",
        "num_samples = 8\n",
        "sample_indices = np.random.choice(len(x_test), num_samples, replace=False)\n",
        "\n",
        "# Create comparison figure\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "print(\"\\\\n🔍 Comparing CIFAR-10 predictions:\")\n",
        "\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    # Get sample image\n",
        "    sample_img = x_test[idx]\n",
        "    true_label = cifar10_classes[y_test[idx][0]]\n",
        "    \n",
        "    # Resize to 224x224 for MobileNetV2\n",
        "    img_resized = tf.image.resize(sample_img, [224, 224])\n",
        "    img_resized = tf.cast(img_resized, tf.uint8)\n",
        "    \n",
        "    # Preprocess for MobileNetV2\n",
        "    x = tf.expand_dims(img_resized, 0)\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    x = preprocess_input(x)\n",
        "    \n",
        "    # Predict with MobileNetV2\n",
        "    preds = model.predict(x, verbose=0)\n",
        "    top_pred = decode_predictions(preds, top=1)[0][0]\n",
        "    \n",
        "    # Display\n",
        "    row, col = i // 4, i % 4\n",
        "    axes[row, col].imshow(sample_img)\n",
        "    axes[row, col].set_title(f'True: {true_label}\\\\nPred: {top_pred[1]}\\\\n({top_pred[2]:.1%})')\n",
        "    axes[row, col].axis('off')\n",
        "    \n",
        "    print(f\"   Image {i+1}: True={true_label}, Predicted={top_pred[1]} ({top_pred[2]:.1%})\")\n",
        "\n",
        "plt.suptitle('CIFAR-10 Predictions with MobileNetV2', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\\\n✅ Exercise 1 completed! CIFAR-10 dataset predictions done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Compare predictions from ResNet and MobileNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Compare ResNet vs MobileNet predictions\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess, decode_predictions as resnet_decode\n",
        "\n",
        "def compare_models(image_path='images/cat.jpeg'):\n",
        "    \"\"\"Compare predictions from ResNet50 and MobileNetV2\"\"\"\n",
        "    \n",
        "    # Load models\n",
        "    resnet_model = ResNet50(weights='imagenet')\n",
        "    mobilenet_model = MobileNetV2(weights='imagenet')\n",
        "    \n",
        "    # Load and preprocess image\n",
        "    img = image.load_img(image_path, target_size=(224, 224))\n",
        "    \n",
        "    # ResNet prediction\n",
        "    x_resnet = image.img_to_array(img)\n",
        "    x_resnet = np.expand_dims(x_resnet, axis=0)\n",
        "    x_resnet = resnet_preprocess(x_resnet)\n",
        "    \n",
        "    resnet_preds = resnet_model.predict(x_resnet, verbose=0)\n",
        "    resnet_top3 = resnet_decode(resnet_preds, top=3)[0]\n",
        "    \n",
        "    # MobileNet prediction\n",
        "    x_mobile = image.img_to_array(img)\n",
        "    x_mobile = np.expand_dims(x_mobile, axis=0)\n",
        "    x_mobile = preprocess_input(x_mobile)\n",
        "    \n",
        "    mobile_preds = mobilenet_model.predict(x_mobile, verbose=0)\n",
        "    mobile_top3 = decode_predictions(mobile_preds, top=3)[0]\n",
        "    \n",
        "    # Display comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title('ResNet50 Predictions:')\n",
        "    for i, (_, label, score) in enumerate(resnet_top3):\n",
        "        plt.text(0, 250 + i*20, f'{i+1}. {label}: {score:.2%}', \n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img)\n",
        "    plt.title('MobileNetV2 Predictions:')\n",
        "    for i, (_, label, score) in enumerate(mobile_top3):\n",
        "        plt.text(0, 250 + i*20, f'{i+1}. {label}: {score:.2%}', \n",
        "                bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"📊 Model Comparison:\")\n",
        "    print(\"\\\\n🔵 ResNet50:\")\n",
        "    for i, (_, label, score) in enumerate(resnet_top3):\n",
        "        print(f\"   {i+1}. {label}: {score:.2%}\")\n",
        "    \n",
        "    print(\"\\\\n🟢 MobileNetV2:\")\n",
        "    for i, (_, label, score) in enumerate(mobile_top3):\n",
        "        print(f\"   {i+1}. {label}: {score:.2%}\")\n",
        "    \n",
        "    return resnet_top3, mobile_top3\n",
        "\n",
        "# Compare models on cat image\n",
        "resnet_results, mobile_results = compare_models()\n",
        "\n",
        "print(\"\\\\n✅ Exercise 2 completed! Model comparison done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Fine-tune a pretrained model on a small custom dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3: Fine-tune a pretrained model on CIFAR-10 dataset\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def create_and_train_fine_tuned_model():\n",
        "    \"\"\"Create and train a fine-tuned model on CIFAR-10\"\"\"\n",
        "    \n",
        "    # Prepare CIFAR-10 data for fine-tuning\n",
        "    # Use only a subset for faster training (first 2 classes: airplane vs automobile)\n",
        "    \n",
        "    # Filter for first 2 classes only\n",
        "    train_mask = (y_train.flatten() < 2)\n",
        "    test_mask = (y_test.flatten() < 2)\n",
        "    \n",
        "    x_train_subset = x_train[train_mask][:1000]  # Use 1000 samples for demo\n",
        "    y_train_subset = y_train[train_mask][:1000]\n",
        "    x_test_subset = x_test[test_mask][:200]     # Use 200 samples for testing\n",
        "    y_test_subset = y_test[test_mask][:200]\n",
        "    \n",
        "    print(f\"📊 Fine-tuning Dataset:\")\n",
        "    print(f\"   • Training samples: {len(x_train_subset)} (airplane vs automobile)\")\n",
        "    print(f\"   • Test samples: {len(x_test_subset)}\")\n",
        "    print(f\"   • Classes: 2 (airplane=0, automobile=1)\")\n",
        "    \n",
        "    # Resize images to 224x224 and normalize\n",
        "    x_train_resized = tf.image.resize(x_train_subset, [224, 224])\n",
        "    x_test_resized = tf.image.resize(x_test_subset, [224, 224])\n",
        "    \n",
        "    x_train_resized = tf.cast(x_train_resized, tf.float32) / 255.0\n",
        "    x_test_resized = tf.cast(x_test_resized, tf.float32) / 255.0\n",
        "    \n",
        "    # Convert labels to categorical\n",
        "    y_train_cat = to_categorical(y_train_subset, 2)\n",
        "    y_test_cat = to_categorical(y_test_subset, 2)\n",
        "    \n",
        "    # Create fine-tuned model\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Add custom classification head\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    predictions = Dense(2, activation='softmax')(x)\n",
        "    \n",
        "    fine_tuned_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "    # Compile model\n",
        "    fine_tuned_model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    print(\"\\\\n🏋️ Training fine-tuned model...\")\n",
        "    \n",
        "    # Train for a few epochs (quick demo)\n",
        "    history = fine_tuned_model.fit(\n",
        "        x_train_resized, y_train_cat,\n",
        "        batch_size=32,\n",
        "        epochs=3,  # Quick training for demonstration\n",
        "        validation_data=(x_test_resized, y_test_cat),\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Evaluate model\n",
        "    test_loss, test_acc = fine_tuned_model.evaluate(x_test_resized, y_test_cat, verbose=0)\n",
        "    \n",
        "    print(f\"\\\\n📊 Fine-tuning Results:\")\n",
        "    print(f\"   • Test accuracy: {test_acc:.2%}\")\n",
        "    print(f\"   • Model successfully trained on CIFAR-10 subset\")\n",
        "    \n",
        "    # Show some predictions\n",
        "    sample_preds = fine_tuned_model.predict(x_test_resized[:4], verbose=0)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    for i in range(4):\n",
        "        axes[i].imshow(x_test_subset[i])\n",
        "        true_class = cifar10_classes[y_test_subset[i][0]]\n",
        "        pred_class = cifar10_classes[np.argmax(sample_preds[i])]\n",
        "        confidence = np.max(sample_preds[i])\n",
        "        \n",
        "        axes[i].set_title(f'True: {true_class}\\\\nPred: {pred_class}\\\\n({confidence:.1%})')\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.suptitle('Fine-tuned Model Predictions on CIFAR-10', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fine_tuned_model, history\n",
        "\n",
        "# Create and train fine-tuned model\n",
        "fine_tuned_model, training_history = create_and_train_fine_tuned_model()\n",
        "\n",
        "print(\"\\\\n✅ Exercise 3 completed! Model fine-tuned on CIFAR-10 dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Visualize activation maps or intermediate feature layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4: Visualize intermediate feature layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def visualize_feature_maps(model, img_tensor, layer_names=None):\n",
        "    \"\"\"Visualize activation maps from intermediate layers\"\"\"\n",
        "    \n",
        "    if layer_names is None:\n",
        "        # Get some interesting layer names from MobileNetV2\n",
        "        layer_names = [\n",
        "            'block_1_expand_relu',  # Early features\n",
        "            'block_6_expand_relu',  # Mid-level features  \n",
        "            'block_13_expand_relu', # High-level features\n",
        "        ]\n",
        "    \n",
        "    # Create models to extract features from intermediate layers\n",
        "    feature_models = {}\n",
        "    for layer_name in layer_names:\n",
        "        try:\n",
        "            layer_output = model.get_layer(layer_name).output\n",
        "            feature_models[layer_name] = Model(inputs=model.input, outputs=layer_output)\n",
        "        except:\n",
        "            print(f\"Layer {layer_name} not found, skipping...\")\n",
        "    \n",
        "    # Get feature maps\n",
        "    fig, axes = plt.subplots(len(feature_models), 4, figsize=(16, 4*len(feature_models)))\n",
        "    if len(feature_models) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, (layer_name, feature_model) in enumerate(feature_models.items()):\n",
        "        # Get activations\n",
        "        activations = feature_model.predict(img_tensor, verbose=0)\n",
        "        \n",
        "        # Show first 4 feature maps\n",
        "        for j in range(min(4, activations.shape[-1])):\n",
        "            ax = axes[i][j] if len(feature_models) > 1 else axes[j]\n",
        "            ax.imshow(activations[0, :, :, j], cmap='viridis')\n",
        "            ax.set_title(f'{layer_name}\\\\nFeature Map {j+1}')\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.suptitle('CNN Feature Map Visualization', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return feature_models\n",
        "\n",
        "# Load image and preprocess for visualization\n",
        "img = image.load_img('images/cat.jpeg', target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "# Visualize feature maps\n",
        "print(\"🔍 Visualizing CNN feature maps...\")\n",
        "feature_models = visualize_feature_maps(model, x)\n",
        "\n",
        "print(\"\\\\n✅ Exercise 4 completed! Feature map visualization done.\")\n",
        "print(\"💡 Different layers show different levels of abstraction:\")\n",
        "print(\"   • Early layers: Edges and simple patterns\")\n",
        "print(\"   • Middle layers: Shapes and textures\")\n",
        "print(\"   • Later layers: Complex objects and concepts\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
